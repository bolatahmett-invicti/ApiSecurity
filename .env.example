# API Discovery & Shadow API Detector Configuration
# ================================================

# HuggingFace API Token (required for LLM-based analysis)
# Get your token at: https://huggingface.co/settings/tokens
HF_TOKEN=hf_your_token_here

# Alternative token name (some libraries use this)
HUGGINGFACE_TOKEN=hf_your_token_here

# HuggingFace Model ID for the agent
# Default: Qwen/Qwen2.5-Coder-32B-Instruct
# Other options:
#   - meta-llama/Llama-3.1-70B-Instruct
#   - mistralai/Mixtral-8x7B-Instruct-v0.1
HF_MODEL_ID=Qwen/Qwen2.5-Coder-32B-Instruct

# AI Enrichment Configuration (v5.0+)
# ====================================

# LLM Provider Selection
# Options: anthropic (default), openai, gemini, bedrock
LLM_PROVIDER=anthropic

# Universal API Key (used if provider-specific key not set)
LLM_API_KEY=

# Provider-Specific API Keys
# ---------------------------

# Anthropic Claude (default provider)
# Get your key at: https://console.anthropic.com/
ANTHROPIC_API_KEY=sk-ant-api03-...

# OpenAI GPT-4
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=sk-...

# Google Gemini
# Get your key at: https://makersuite.google.com/app/apikey
GOOGLE_API_KEY=...

# AWS Bedrock (requires both access key and secret key)
# Configure via AWS CLI or set these variables
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_REGION=us-east-1

# Model Configuration (provider-specific)
# ----------------------------------------

# General model setting (overrides provider defaults)
LLM_MODEL=

# Provider defaults:
#   Anthropic: claude-sonnet-4-5-20250929
#   OpenAI: gpt-4-turbo
#   Gemini: gemini-1.5-pro
#   Bedrock: anthropic.claude-3-5-sonnet-20241022-v2:0

# Maximum tokens for LLM responses
LLM_MAX_TOKENS=4096

# Per-Agent Model Selection (Cost Optimization)
# ----------------------------------------------
# Different agents can use different models for cost optimization
# Simple tasks (auth/dependency detection) → Haiku (cheap)
# Complex tasks (OpenAPI/payload generation) → Sonnet (quality)
#
# Cost comparison (per 100 endpoints):
#   All Sonnet: ~$3.95-6.69
#   Optimized (Haiku for Auth/Dependency): ~$3.84-6.58 (15-20% savings)
#
# Available models:
#   Anthropic: claude-sonnet-4-5-20250929, claude-haiku-4-5-20251001
#   OpenAI: gpt-4-turbo, gpt-4o-mini
#   Gemini: gemini-1.5-pro, gemini-1.5-flash

# OpenAPI enrichment (requires code analysis - use Sonnet)
LLM_MODEL_OPENAPI=claude-sonnet-4-5-20250929

# Authentication detection (simple pattern matching - use Haiku)
LLM_MODEL_AUTH=claude-haiku-4-5-20251001

# Payload generation (use Sonnet for quality, or Haiku for cost)
LLM_MODEL_PAYLOADS=claude-sonnet-4-5-20250929

# Dependency graph (simple relationship detection - use Haiku)
LLM_MODEL_DEPENDENCY=claude-haiku-4-5-20251001

# Enrichment Configuration
# ------------------------

# Enable AI enrichment by default (can be overridden with --ai-enrich flag)
ENABLE_AI_ENRICHMENT=false

# Cache directory for enrichment results
ENRICHMENT_CACHE_DIR=./.cache/enrichment

# Cache TTL in seconds (default: 7 days = 604800)
ENRICHMENT_CACHE_TTL=604800

# Maximum concurrent AI agent executions
ENRICHMENT_MAX_WORKERS=3

# Enabled agents (comma-separated)
# Options: openapi_enrichment, auth_flow_detector, payload_generator, dependency_graph
ENRICHMENT_AGENTS=openapi_enrichment,auth_flow_detector,payload_generator,dependency_graph

# Fallback to basic export if AI enrichment fails
ENRICHMENT_FALLBACK_ENABLED=true

# Batch Processing (Cost Optimization - Phase 2)
# -----------------------------------------------
# Process multiple endpoints per LLM call instead of one-by-one
# Reduces API calls and costs by ~25-30%
#
# Cost comparison (100 endpoints):
#   Without batching: 100 LLM calls for OpenAPI + 40 calls for payloads = 140 calls
#   With batching: 5 LLM calls for OpenAPI + 3 calls for payloads = 8 calls (~94% reduction in API calls)
#   Total savings: ~25-30% in token costs due to shared context

# Enable batch processing (default: true)
ENABLE_BATCHING=true

# OpenAPI enrichment batch size (default: 20)
# Groups endpoints by HTTP method before processing
# Recommended: 15-25 (larger = more savings, but may reduce quality)
OPENAPI_BATCH_SIZE=20

# Payload generation batch size (default: 15)
# Groups endpoints by resource type before processing
# Recommended: 10-20
PAYLOAD_BATCH_SIZE=15

# Fall back to per-endpoint processing if batch fails (default: true)
BATCH_FALLBACK_PER_ENDPOINT=true

# Hybrid Architecture (Cost Optimization - Phase 1)
# --------------------------------------------------
# Use deterministic Python code for pattern-based tasks, reserve LLM for complex analysis
# This approach reduces costs by 70-85% compared to 100% LLM enrichment
#
# Cost comparison (100 endpoints):
#   100% LLM (baseline): ~$6.69
#   Hybrid (Phase 1): ~$1.00-2.00 (70-85% savings!)
#
# How it works:
#   - Parameters: Extracted from routes using regex ({id}, <int:id>, :id) - NO LLM
#   - HTTP methods: Mapped to OpenAPI operations using RFC standards - NO LLM
#   - Status codes: Extracted from source code + standard descriptions - NO LLM
#   - Security payloads: Static OWASP templates - NO LLM
#   - Complex analysis: Still uses LLM (business logic, descriptions)
#
# Philosophy: "Python for structure, LLM for semantics"

# Enable hybrid architecture (default: true)
USE_DETERMINISTIC_ENRICHMENT=true

# Extract parameters from route patterns (default: true)
# Detects: /users/{id}, /api/<int:user_id>, /posts/:slug
# Accuracy: 90-95%
DETERMINISTIC_PARAMETERS=true

# Extract status codes from source code (default: true)
# Detects: return ..., 200; raise HTTPException(status_code=404)
# Accuracy: 95%
DETERMINISTIC_STATUS_CODES=true

# Map HTTP methods to OpenAPI operations (default: true)
# GET=no body, POST=body required, etc. (100% accurate - RFC standards)
DETERMINISTIC_HTTP_METHODS=true

# Confidence threshold for using LLM fallback (default: 0.7)
# If Python extraction confidence < 70%, fall back to LLM
# Range: 0.0-1.0 (lower = more aggressive Python usage)
DETERMINISTIC_CONFIDENCE_THRESHOLD=0.7

# Hybrid Architecture - Phase 2: AST-Based Analysis
# --------------------------------------------------
# Use Python AST module for type hints, decorators, and docstrings
# Additional cost reduction: 15% (total: 85% from baseline)
#
# Cost comparison (100 endpoints):
#   Phase 1 only: ~$1.00-2.00 (70-85% savings)
#   Phase 1 + 2: ~$0.80-1.20 (85% savings!)
#
# How Phase 2 works:
#   - Type hints: Extract from function signatures (int→integer, List[str]→array)
#   - Decorators: Detect auth patterns (@jwt_required → bearerAuth)
#   - Docstrings: Extract descriptions (Google/NumPy/Sphinx formats)

# Extract type hints from function signatures (default: true)
# Converts: def get_user(user_id: int) → {"user_id": {"type": "integer"}}
# Accuracy: 95%+ (direct type introspection)
DETERMINISTIC_TYPE_HINTS=true

# Detect auth patterns from decorators (default: true)
# Converts: @jwt_required → {"security": [{"bearerAuth": []}]}
# Accuracy: 90%+ (decorators follow conventions)
DETERMINISTIC_DECORATORS=true

# Extract descriptions from docstrings (default: true)
# Supports: Google, NumPy, Sphinx, and plain text formats
# Accuracy: 95%+ (docstrings are structured text)
DETERMINISTIC_DOCSTRINGS=true

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# Path Security - Restrict which directories can be scanned
# Leave empty to allow any non-system directory
# Example: ALLOWED_SCAN_DIRS=/workspace,/app,/home/user/projects
ALLOWED_SCAN_DIRS=

# Enable scanning of symbolic links (default: false for security)
# Set to "true" to allow scanning directories that are symlinks
ENABLE_SYMLINK_SCAN=false
